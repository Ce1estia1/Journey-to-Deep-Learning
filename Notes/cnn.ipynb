{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 卷积神经网络 基础理论篇"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "readme first:\n",
    "本文以及其原著的阅读需要一定的深度学习基础，我建议先去看李宏毅老师的机器学习课程的前两次内容，了解了什么是batch、epoch、iteration，知道什么是relu、sigmoid，知道什么是optimizer、损失函数之后，再来学习本文，实际上，卷积神经网络就是神经网络的一路特殊模型，问道有先后，不可急于求成~~\n",
    "课程链接：https://www.bilibili.com/video/BV1Wv411h7kN/?spm_id_from=333.337.search-card.all.click\n",
    "列一个应该知道的知识清单：\n",
    "周期：\n",
    "epoch\n",
    "iteration/update\n",
    "数据：\n",
    "batch\n",
    "dataloader\n",
    "dataset\n",
    "training set\n",
    "testing set\n",
    "validation set\n",
    "函数：\n",
    "Loss function\n",
    "Activation function: ReLU, sigmoid, hard sigmoid\n",
    "Object function（how is it related to Loss function）\n",
    "算法：\n",
    "backward propagation\n",
    "名词：\n",
    "hidden layer\n",
    "input layer\n",
    "output layer\n",
    "矩阵相关知识\n",
    "多元微分\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.卷积神经网络基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 发展历程（孙金夷写2023/7/11）\n",
    "  1959年，加拿大神经科学家 David H.Hubel 和 Torsten 提出猫的初级视皮层单个神经元的“感受野”（receptive field）概念\n",
    "  1962年，发现猫的视觉中枢里存在感受野、双目视觉和其他功能结构，标志着神经网络结构首次在大脑视觉系统中被发现\n",
    "\n",
    "  1980年，日本科学家福岛邦彦在上述二位的基础上，提出一种 层级化 的多层人工神经网络，即“神经认知”（neurocognitron，以下简称neuro）\n",
    "neuro两种最重要的组成单元：S-cell 和 C-cell，其中，Scell用于抽取局部特征，后者用于抽象和容错，与如今CNN中的卷积层（convolution layer）\n",
    "和汇合层（或称 池化层 pooling layer）一一对应\n",
    "\n",
    "\n",
    "![Neurocognitron](./gra/neuroco.png)\n",
    "\n",
    "  1998年，Yann LeCun等人提出基于梯度学习的卷积神经网络算法，并将其用于手写体识别，在当时的条件下，错误率仅为1%\n",
    "![LeCun的模型](./gra/LNET.png)\n",
    "\n",
    "2012年，在ImageNet图像分类竞赛四周年，Geoffrey E.Hinton等人凭借CNN Alex-Net 并以超过第二名12%的准确而率夺魁，学界愕然。\n",
    "\n",
    "2015年，在改进了激活函数（activation function）后，卷积神经网络在ImageNet数据集上的性能第一次超过了人类预测的错误率（4.94:5.1）\n",
    "\n",
    "实际上，LeNet与Alex-net结构几乎毫无差异，数据和硬件设备才是革新的主引擎。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2基本结构（孙金夷写2023/7/11）\n",
    "\n",
    "结构：层次模型\n",
    "输入：原始数据（raw data），RGB图像、音频数据等\n",
    "操作：\n",
    "    &nbsp; &nbsp; &nbsp; 前馈运算（feed forward）-将高层语义从原始数据输入层抽取出来，逐层抽象，操作包括：\n",
    "     &nbsp; &nbsp; &nbsp;Convolution, pooling, non-linear activation function映射（不同类型操作称为层，例如我这一个区域做卷积，就叫卷积层，那一个区域做池化，叫池化 &nbsp; &nbsp; &nbsp;层，最后一层将目标任务形式化为目标函数）\n",
    "     &nbsp; &nbsp; &nbsp;反馈运算（back forward）-计算真实值与预测值的误差，通过反向传播算法（back-propagation algorithm）将误差活损失信息由后往前传播，并用以调整参数\n",
    "&nbsp; &nbsp; &nbsp;前馈与反馈运算周而复始，以至于模型收敛，从而达到训练模型的目的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3前馈运算（孙金夷写2023/7/11）\n",
    "没什么好说的，就是从第一层输入层到最后一层目标函数层走一遍流程。\n",
    "特别提一下对最后一层的利用，如果我们现在做手写体数字识别，最后一层就会得到一个十维向量$X^l$\n",
    "$X^l$的第i维表示我们要识别的数字为数字i的概率，那么显然，我们可以认为概率值最大的那一维度对应的序号，就是预测的数字\n",
    " $$answer = argmax (x^l_i)$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4反馈运算（孙金夷写2023/7/11）\n",
    "这一部分可以看这个书去详细了解，反馈的微分运算都已被很完善地实现了连接：http://neuralnetworksanddeeplearning.com/index.html\n",
    "\n",
    "从凸优化地角度来说，显然CNN是一个非凸函数，而且异常复杂，这便带来了优化求解的困难。我们一般采用随机梯度下降法（stochastic gradient descent， SGD）和误差反向传播进行迭代（error back propagation）。\n",
    "\n",
    "和普通神经网络大同小异，重点不在于此。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
